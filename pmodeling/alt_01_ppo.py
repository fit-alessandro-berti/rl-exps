import os
import json
import random
import torch
import re
import requests
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForCausalLMWithValueHead
from trl import PPOConfig, PPOTrainer
from tqdm import tqdm
from typing import List

# ---------------------------------------------------------------------------#
# 1. Configuration & Hyperparameters                                          #
# ---------------------------------------------------------------------------#
SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# --- Directory and Model Configuration ---
TRAIN_DATA_DIR = "training"
MODEL_NAME = "Qwen/Qwen2.5-Coder-3B"
OUTPUT_DIR = "ppo_qwen_powl_generator_openai_reward"

# --- Training Hyperparameters ---
MAX_PROMPT_TOKENS = 4096
MAX_NEW_TOKENS = 4096  # Max tokens for the generated completion
BATCH_SIZE = 1         # Number of prompts to process at once
LEARNING_RATE = 1.41e-5
PPO_EPOCHS = 4
INIT_KL_COEF = 0.1
MAX_TRAINING_STEPS = 1000
MAX_DATASET_SAMPLES = 500
GRADIENT_ACCUMULATION_STEPS = 4

# --- Logging and Saving Configuration ---
LOGGING_INTERVAL = 1
SAVE_INTERVAL = 100

# --- OpenAI API Configuration ---
OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"
OPENAI_MODEL = "gpt-4.1-mini"


# ---------------------------------------------------------------------------#
# 2. Prompt and Data Loading                                                  #
# ---------------------------------------------------------------------------#

def get_powl_prompt(description: str, activities: List[str]) -> str:
    """
    Formats the prompt with the process description and activities.
    """
    activities_str = ", ".join([f"'{act}'" for act in activities])
    return f"""Generate a POWL model for the following process, saving the final result in the variable 'root'.

A partially ordered workflow language (POWL) is a partially ordered graph representation of a process, extended with control-flow operators for modeling choice and loop structures. There are four types of POWL models:
- an activity (identified by its label, e.g., 'M' identifies the activity M). Silent activities with empty labels (tau labels) are also supported.
- a choice of other POWL models (exclusive choice: X(A, B)).
- a loop node (* (A, B)): execute A, then choose to exit or execute B then A again, repeated until exit.
- a partial order: PO=(nodes={{...}}, order={{...}}), where order is a set of source-->target dependencies; unconnected nodes are concurrent.

Example code:
python
import pm4py
from pm4py.objects.powl.obj import StrictPartialOrder, OperatorPOWL, Transition, SilentTransition
from pm4py.objects.process_tree.obj import Operator
A = Transition(label='A')
B = Transition(label='B')
C = Transition(label='C')
skip = SilentTransition()
loop = OperatorPOWL(operator=Operator.LOOP, children=[A, B])
xor = OperatorPOWL(operator=Operator.XOR, children=[C, skip])
root = StrictPartialOrder(nodes=[loop, xor])
root.order.add_edge(loop, xor)


NOW, generate the POWL model for the process below.
DESCRIPTION: {description}
ACTIVITIES (use these exactly, same names): [{activities_str}]

Respond with valid Python code only, defining 'root'.
"""


def load_limited_dataset(data_dir: str, max_samples: int) -> Dataset:
    """
    Loads a limited number of samples into a standard Hugging Face Dataset.
    For PPO, we only need the prompt, as the completion is generated by the model.
    """
    desc_folder = os.path.join(data_dir, "textual_descriptions")
    if not os.path.exists(desc_folder):
        raise FileNotFoundError(f"Data directory not found in '{data_dir}'.")

    file_names = [f for f in os.listdir(desc_folder) if f.endswith('.json')]
    random.shuffle(file_names)

    data_list = []
    for file_name in file_names[:max_samples]:
        json_path = os.path.join(desc_folder, file_name)
        with open(json_path, 'r', encoding='utf-8') as f:
            desc_data = json.load(f)
        prompt = get_powl_prompt(desc_data["description"], desc_data["activities"])
        data_list.append({"prompt": prompt})

    print(f"Loaded {len(data_list)} samples into the dataset.")
    return Dataset.from_list(data_list)


# The collator tokenizes the prompts.
def collator(data):
    return {key: [d[key] for d in data] for key in data[0]}


dataset = load_limited_dataset(TRAIN_DATA_DIR, max_samples=MAX_DATASET_SAMPLES)


# ---------------------------------------------------------------------------#
# 3. REWARD FUNCTION (using OpenAI API)                                       #
# ---------------------------------------------------------------------------#

def get_openai_grading_prompt(original_prompt: str, completions: List[str]) -> str:
    """
    Creates the prompt for the OpenAI grading model.
    """
    prompt_intro = f"""
You are an expert in process modeling. Your task is to evaluate Python code snippets that generate POWL models based on a given prompt.

The original prompt was:
---
{original_prompt}
---

I have received {len(completions)} responses. Please evaluate each response based on its correctness, completeness, and adherence to the prompt's requirements.
Provide a grade for each response on a scale from -1.0 to 1.0, where:
- 1.0: The generated code is perfect and accurately models the process description.
- 0.1 to 0.9: The code is acceptable but may have minor errors or inaccuracies.
- 0.0: The code is syntactically correct but functionally wrong or completely misses the point.
- -1.0: The code is terrible, syntactically incorrect, or completely irrelevant.

Here are the responses to grade:
"""
    responses_section = ""
    for i, response in enumerate(completions):
        responses_section += f"\n--- RESPONSE {i + 1} ---\npython\n{response}\n"

    prompt_outro = f"""
Please provide your evaluation in a single JSON object. The JSON should have a single key "grades", which is a list of floating-point numbers corresponding to each response. The number of grades in the list must be exactly {len(completions)}.

Example format:
{{
  "grades": [0.8, -0.5, 1.0]
}}

Now, provide the JSON output for the responses above.
"""
    return prompt_intro + responses_section + prompt_outro


def get_rewards(prompts: List[str], completions: List[str]) -> List[torch.FloatTensor]:
    """
    Calculates rewards by getting grades from the OpenAI API.
    """
    BAD_REWARD = -1.0
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        print("ERROR: OPENAI_API_KEY environment variable not set.")
        return [torch.tensor(BAD_REWARD)] * len(completions)

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    # For efficiency, we send all completions to be graded in one API call.
    # We use the first prompt as the reference, assuming prompts in a batch are similar.
    grading_prompt = get_openai_grading_prompt(prompts[0], completions)

    payload = {
        "model": OPENAI_MODEL,
        "messages": [{"role": "user", "content": grading_prompt}],
        "response_format": {"type": "json_object"},
        "temperature": 0.0,
    }

    try:
        response = requests.post(OPENAI_API_URL, headers=headers, json=payload, timeout=60)
        response.raise_for_status()
        response_data = response.json()
        grades_str = response_data.get("choices", [{}])[0].get("message", {}).get("content", "{}")
        grades_json = json.loads(grades_str)
        grades = grades_json.get("grades")

        if grades is None or not isinstance(grades, list) or len(grades) != len(completions):
            print(f"ERROR: OpenAI response was malformed. Received: {grades_str}")
            return [torch.tensor(BAD_REWARD)] * len(completions)

        print(f"--- GRADES FOR STEP: {grades} ---")
        return [torch.tensor(float(g)) for g in grades]

    except requests.RequestException as e:
        print(f"ERROR: OpenAI API request failed: {e}")
        return [torch.tensor(BAD_REWARD)] * len(completions)
    except (json.JSONDecodeError, KeyError, TypeError, IndexError) as e:
        print(f"ERROR: Failed to parse OpenAI response: {e}. Response text: {response.text}")
        return [torch.tensor(BAD_REWARD)] * len(completions)


# ---------------------------------------------------------------------------#
# 4. Model, Tokenizer, and Trainer Setup                                      #
# ---------------------------------------------------------------------------#

# PPOConfig contains all the hyperparameters for PPO training
config = PPOConfig(
    learning_rate=LEARNING_RATE,
    batch_size=BATCH_SIZE,
    ppo_epochs=PPO_EPOCHS,
    init_kl_coef=INIT_KL_COEF,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
    log_with=None,  # We will do manual logging
)

# Load the model and tokenizer
model_load_path = MODEL_NAME
if os.path.isdir(OUTPUT_DIR):
    checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith("checkpoint-")]
    if checkpoints:
        latest_checkpoint = max(checkpoints, key=lambda d: int(re.search(r'-(\d+)$', d).group(1)))
        model_load_path = os.path.join(OUTPUT_DIR, latest_checkpoint)
        print(f"‚úÖ Resuming training from model checkpoint: {model_load_path}")
    else:
        print(f"üèÅ No checkpoint found in '{OUTPUT_DIR}'. Starting from base model: {MODEL_NAME}")
else:
    print(f"üèÅ Output directory '{OUTPUT_DIR}' not found. Starting from base model: {MODEL_NAME}")
    os.makedirs(OUTPUT_DIR, exist_ok=True)


# The PPOTrainer from TRL expects a model with a value head for PPO training.
# `AutoModelForCausalLMWithValueHead` automatically adds this.
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_load_path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side="left", use_fast=False)
tokenizer.pad_token = tokenizer.eos_token

# Instantiate the PPOTrainer
ppo_trainer = PPOTrainer(
    #config=config,
    model=model,
    ref_model=None, # The value head model combines policy and value functions
    #tokenizer=tokenizer,
    dataset=dataset,
    data_collator=collator
)

# Generation settings for creating completions
generation_kwargs = {
    "max_new_tokens": MAX_NEW_TOKENS,
    "min_new_tokens": 50,
    "pad_token_id": tokenizer.eos_token_id,
    "eos_token_id": tokenizer.eos_token_id,
    "do_sample": True,
    "top_k": 50,
    "top_p": 0.95,
}

# ---------------------------------------------------------------------------#
# 5. Training Execution                                                       #
# ---------------------------------------------------------------------------#

print("\n--- Starting PPO Training with OpenAI-based Rewards ---")
print(f"Logging diagnostics every {LOGGING_INTERVAL} steps.")
print(f"Saving model checkpoint every {SAVE_INTERVAL} steps.")

for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    if step >= MAX_TRAINING_STEPS:
        break

    prompt_texts = batch["prompt"]
    query_tensors = [tokenizer.encode(p, return_tensors="pt").to(ppo_trainer.accelerator.device) for p in prompt_texts]

    # Generate model completions
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_texts = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)

    # Get rewards from the external API
    rewards = get_rewards(prompt_texts, response_texts)

    # Run PPO optimization step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)

    # Log statistics
    if (step + 1) % LOGGING_INTERVAL == 0:
        log_stats = {
            "step": step + 1,
            "mean_reward": stats["ppo/returns/mean"],
            "kl_divergence": stats["ppo/kl/mean"],
            "loss/policy": stats["ppo/loss/policy"],
            "loss/value": stats["ppo/loss/value"],
        }
        print(f"\nStep {step+1}/{MAX_TRAINING_STEPS}: {log_stats}")

    # Save model checkpoint
    if (step + 1) % SAVE_INTERVAL == 0:
        checkpoint_dir = os.path.join(OUTPUT_DIR, f"checkpoint-{step+1}")
        ppo_trainer.save_pretrained(checkpoint_dir)
        print(f"\n‚úÖ Saved checkpoint to {checkpoint_dir}")


print("\n--- Training Finished ---")

final_save_path = os.path.join(OUTPUT_DIR, "final_model")
ppo_trainer.save_pretrained(final_save_path)
print(f"Final model and tokenizer saved to {final_save_path}")
